#!/usr/bin/env python3
"""
Dhamma.kr ì „ì²´ í¬ë¡¤ëŸ¬ - ëª¨ë“  ê¸€ì„ ì˜ˆìœ PDFë¡œ ì €ì¥
"""

import requests
from bs4 import BeautifulSoup
from weasyprint import HTML
import urllib3
import os
import re
import time

# SSL ê²½ê³  ë¬´ì‹œ
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# í•œê¸€ í°íŠ¸ ê²½ë¡œ
FONT_PATH = "/Users/jinseulpark/Desktop/github/jsks_app/scraper/fonts/NanumGothic.ttf"

def get_all_post_links(base_url, max_pages=10):
    """ëª¨ë“  ê¸€ì˜ ë§í¬ ìˆ˜ì§‘"""
    print("ğŸ“¡ ê¸€ ëª©ë¡ ìˆ˜ì§‘ ì¤‘...")

    post_links = []
    page = 1

    while page <= max_pages:
        url = f"{base_url}?paged={page}" if page > 1 else base_url
        print(f"   í˜ì´ì§€ {page}/{max_pages} í™•ì¸ ì¤‘...")

        try:
            response = requests.get(url, verify=False, timeout=10)
            if response.status_code != 200:
                print(f"   âš ï¸  í˜ì´ì§€ {page} ì ‘ê·¼ ì‹¤íŒ¨ (status: {response.status_code})")
                break

            soup = BeautifulSoup(response.content, 'html.parser')

            # dhamma.kr ì „ìš©: <div class="post"> ì•ˆì˜ <a class="title"> ì°¾ê¸°
            posts = soup.find_all('div', class_='post')

            if not posts:
                print(f"   âš ï¸  í˜ì´ì§€ {page}ì—ì„œ ê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                break

            found_count = 0
            for post in posts:
                title_link = post.find('a', class_='title')
                if title_link and title_link.get('href'):
                    href = title_link['href']
                    if href not in post_links:
                        post_links.append(href)
                        found_count += 1

            print(f"   âœ… {found_count}ê°œ ê¸€ ë°œê²¬ (ëˆ„ì : {len(post_links)}ê°œ)")

            page += 1

            # ì„œë²„ ë¶€í•˜ ë°©ì§€
            time.sleep(0.5)

        except Exception as e:
            print(f"   âš ï¸  í˜ì´ì§€ {page} ì˜¤ë¥˜: {e}")
            page += 1
            continue

    print(f"\nâœ… ì´ {len(post_links)}ê°œì˜ ê¸€ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
    return post_links

def scrape_post_content(url):
    """ê°œë³„ ê¸€ ë‚´ìš© í¬ë¡¤ë§"""
    try:
        response = requests.get(url, verify=False, timeout=10)
        soup = BeautifulSoup(response.content, 'html.parser')

        # ì œëª©
        title = soup.find('h2')
        title_text = title.get_text(strip=True) if title else "ì œëª© ì—†ìŒ"

        # ë‚´ìš©
        post_div = soup.find('div', class_='post')
        if post_div:
            paragraphs = post_div.find_all('p')
            content_paragraphs = paragraphs[1:-1] if len(paragraphs) > 2 else paragraphs
            content_html = '\n'.join([f'<p>{p.decode_contents()}</p>' for p in content_paragraphs if p.get_text(strip=True)])
        else:
            content_html = ""

        # ë‚ ì§œ
        date_span = soup.find('span', class_='date')
        date_text = date_span.get_text(strip=True) if date_span else ""

        return {
            'title': title_text,
            'content_html': content_html,
            'date': date_text,
            'url': url
        }

    except Exception as e:
        print(f"âš ï¸  í¬ë¡¤ë§ ì˜¤ë¥˜ ({url}): {e}")
        return None

def create_beautiful_pdf(post_data, output_dir):
    """WeasyPrintë¡œ ì˜ˆìœ PDF ìƒì„±"""
    if not post_data:
        return False

    try:
        # HTML í…œí”Œë¦¿ ìƒì„±
        html_content = f"""
<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <title>{post_data['title']}</title>
    <style>
        @font-face {{
            font-family: 'NanumGothic';
            src: url('file://{FONT_PATH}');
        }}

        body {{
            font-family: 'NanumGothic', serif;
            font-size: 12pt;
            line-height: 1.8;
            color: #333;
            margin: 2cm;
            max-width: 800px;
        }}

        h1 {{
            font-size: 24pt;
            font-weight: bold;
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 20px;
        }}

        .meta {{
            font-size: 10pt;
            color: #7f8c8d;
            margin-bottom: 30px;
            padding: 10px;
            background-color: #ecf0f1;
            border-left: 4px solid #3498db;
        }}

        .content {{
            text-align: justify;
            word-break: keep-all;
        }}

        .content p {{
            margin-bottom: 1em;
            text-indent: 1em;
        }}

        .footer {{
            margin-top: 50px;
            padding-top: 20px;
            border-top: 1px solid #bdc3c7;
            font-size: 9pt;
            color: #95a5a6;
            text-align: center;
        }}

        @page {{
            size: A4;
            margin: 2cm;

            @bottom-right {{
                content: counter(page) " / " counter(pages);
                font-family: 'NanumGothic';
                font-size: 9pt;
                color: #7f8c8d;
            }}
        }}
    </style>
</head>
<body>
    <h1>{post_data['title']}</h1>

    <div class="meta">
        <strong>ë‚ ì§œ:</strong> {post_data['date']}<br>
        <strong>ì¶œì²˜:</strong> {post_data['url']}
    </div>

    <div class="content">
        {post_data['content_html']}
    </div>

    <div class="footer">
        ë³¸ ë¬¸ì„œëŠ” dhamma.krì—ì„œ ìˆ˜ì§‘í•œ ë‚´ìš©ì…ë‹ˆë‹¤.
    </div>
</body>
</html>
"""

        # íŒŒì¼ëª… ìƒì„±
        safe_title = re.sub(r'[^\w\s-]', '', post_data['title'])[:50]
        filename = f"{safe_title}.pdf"
        filepath = os.path.join(output_dir, filename)

        # PDF ìƒì„±
        HTML(string=html_content).write_pdf(filepath)

        return True

    except Exception as e:
        print(f"âš ï¸  PDF ìƒì„± ì˜¤ë¥˜: {e}")
        return False

def main():
    base_url = "http://www.dhamma.kr/wp/"
    output_dir = "/Users/jinseulpark/Desktop/github/jsks_app/scraper/pdfs"

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(output_dir, exist_ok=True)

    print("ğŸš€ Dhamma.kr í¬ë¡¤ë§ ì‹œì‘\\n")
    print("ì˜µì…˜ì„ ì„ íƒí•˜ì„¸ìš”:")
    print("1. í…ŒìŠ¤íŠ¸ (ìµœê·¼ 10ê°œ í˜ì´ì§€)")
    print("2. ì „ì²´ í¬ë¡¤ë§ (3,368 í˜ì´ì§€, ì•½ 6ì‹œê°„ ì†Œìš”)")

    choice = input("\nì„ íƒ (1 or 2): ").strip()

    if choice == '1':
        max_pages = 10
        print(f"\nğŸ“Œ í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ìµœê·¼ {max_pages}ê°œ í˜ì´ì§€ í¬ë¡¤ë§")
    elif choice == '2':
        max_pages = 3368
        print(f"\nğŸ“Œ ì „ì²´ ëª¨ë“œ: {max_pages}ê°œ í˜ì´ì§€ í¬ë¡¤ë§ (ì˜ˆìƒ ì‹œê°„: 6ì‹œê°„)")
        confirm = input("ê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").strip().lower()
        if confirm != 'y':
            print("ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            return
    else:
        print("ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")
        return

    # 1. ëª¨ë“  ê¸€ ë§í¬ ìˆ˜ì§‘
    post_links = get_all_post_links(base_url, max_pages)

    if not post_links:
        print("âŒ ê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 2. ê° ê¸€ í¬ë¡¤ë§ ë° PDF ìƒì„±
    print(f"\nğŸ“„ PDF ìƒì„± ì‹œì‘...\\n")

    success_count = 0
    fail_count = 0

    for i, link in enumerate(post_links, 1):
        print(f"[{i}/{len(post_links)}] {link}")

        post_data = scrape_post_content(link)
        if post_data and create_beautiful_pdf(post_data, output_dir):
            success_count += 1
            print(f"   âœ… PDF ì €ì¥ ì™„ë£Œ: {post_data['title'][:30]}...")
        else:
            fail_count += 1
            print(f"   âŒ ì‹¤íŒ¨")

        # ì„œë²„ ë¶€í•˜ ë°©ì§€
        time.sleep(1)

        # 10ê°œë§ˆë‹¤ ì§„í–‰ ìƒí™© ì €ì¥
        if i % 10 == 0:
            print(f"\nğŸ“Š ì§„í–‰ ìƒí™©: {success_count} ì„±ê³µ, {fail_count} ì‹¤íŒ¨\n")

    print(f"\nâœ¨ ì™„ë£Œ! {success_count}/{len(post_links)}ê°œì˜ PDF ìƒì„±ë¨")
    print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {output_dir}")

if __name__ == "__main__":
    main()

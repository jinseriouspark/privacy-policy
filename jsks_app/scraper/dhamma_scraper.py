#!/usr/bin/env python3
"""
Dhamma.kr ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ëŸ¬ - ëª¨ë“  ê¸€ì„ PDFë¡œ ì €ì¥
"""

import requests
from bs4 import BeautifulSoup
from fpdf import FPDF
import urllib3
import os
from datetime import datetime
import re

# SSL ê²½ê³  ë¬´ì‹œ
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class DhammaPDF(FPDF):
    """í•œê¸€ ì§€ì› PDF í´ë˜ìŠ¤"""

    def __init__(self):
        super().__init__()
        # í•œê¸€ í°íŠ¸ ì„¤ì • (ì‹œìŠ¤í…œ í°íŠ¸ ì‚¬ìš©)
        # Mac: /System/Library/Fonts/AppleSDGothicNeo.ttc
        # Windows: C:/Windows/Fonts/malgun.ttf
        try:
            self.add_font('NanumGothic', '', '/System/Library/Fonts/AppleSDGothicNeo.ttc', uni=True)
            self.font_available = True
        except:
            print("âš ï¸  í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ í°íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            self.font_available = False

    def header(self):
        if self.font_available:
            self.set_font('NanumGothic', '', 12)
        self.cell(0, 10, 'Dhamma.kr', 0, 1, 'C')
        self.ln(5)

    def footer(self):
        self.set_y(-15)
        if self.font_available:
            self.set_font('NanumGothic', '', 8)
        self.cell(0, 10, f'Page {self.page_no()}', 0, 0, 'C')

def get_all_post_links(base_url):
    """ëª¨ë“  ê¸€ì˜ ë§í¬ ìˆ˜ì§‘"""
    print("ğŸ“¡ ê¸€ ëª©ë¡ ìˆ˜ì§‘ ì¤‘...")

    post_links = []
    page = 1
    max_pages = 3368  # ì‹¤ì œ ì‚¬ì´íŠ¸ì— 3,368 í˜ì´ì§€ ì¡´ì¬

    while page <= max_pages:
        url = f"{base_url}?paged={page}" if page > 1 else base_url
        print(f"   í˜ì´ì§€ {page}/{max_pages} í™•ì¸ ì¤‘...")

        try:
            response = requests.get(url, verify=False, timeout=10)
            if response.status_code != 200:
                print(f"   âš ï¸  í˜ì´ì§€ {page} ì ‘ê·¼ ì‹¤íŒ¨ (status: {response.status_code})")
                break

            soup = BeautifulSoup(response.content, 'html.parser')

            # dhamma.kr ì „ìš©: <div class="post"> ì•ˆì˜ <a class="title"> ì°¾ê¸°
            posts = soup.find_all('div', class_='post')

            if not posts:
                print(f"   âš ï¸  í˜ì´ì§€ {page}ì—ì„œ ê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                break

            found_count = 0
            for post in posts:
                title_link = post.find('a', class_='title')
                if title_link and title_link.get('href'):
                    href = title_link['href']
                    if href not in post_links:
                        post_links.append(href)
                        found_count += 1

            print(f"   âœ… {found_count}ê°œ ê¸€ ë°œê²¬ (ëˆ„ì : {len(post_links)}ê°œ)")

            page += 1

            # ì„œë²„ ë¶€í•˜ ë°©ì§€ (í˜ì´ì§€ ìˆ˜ì§‘ ê°„ ë”œë ˆì´)
            import time
            time.sleep(0.5)

        except Exception as e:
            print(f"   âš ï¸  í˜ì´ì§€ {page} ì˜¤ë¥˜: {e}")
            # ì—ëŸ¬ê°€ ë°œìƒí•´ë„ ë‹¤ìŒ í˜ì´ì§€ ì‹œë„
            page += 1
            continue

    print(f"\nâœ… ì´ {len(post_links)}ê°œì˜ ê¸€ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
    return post_links

def scrape_post_content(url):
    """ê°œë³„ ê¸€ ë‚´ìš© í¬ë¡¤ë§"""
    try:
        response = requests.get(url, verify=False, timeout=10)
        soup = BeautifulSoup(response.content, 'html.parser')

        # ì œëª© ì°¾ê¸° (dhamma.krì€ ë‹¨ìˆœíˆ <h2> ì‚¬ìš©)
        title = soup.find('h2')
        title_text = title.get_text(strip=True) if title else "ì œëª© ì—†ìŒ"

        # ë‚´ìš© ì°¾ê¸° (post div ì•ˆì˜ ëª¨ë“  p íƒœê·¸)
        post_div = soup.find('div', class_='post')
        if post_div:
            paragraphs = post_div.find_all('p')
            # ì²«ë²ˆì§¸ì™€ ë§ˆì§€ë§‰ p íƒœê·¸ëŠ” ì œì™¸ (ì œëª©ê³¼ ê´€ë ¨ ê¸€ ë§í¬)
            content_paragraphs = paragraphs[1:-1] if len(paragraphs) > 2 else paragraphs
            content_text = '\n\n'.join([p.get_text(strip=True) for p in content_paragraphs if p.get_text(strip=True)])
        else:
            content_text = ""

        # ë‚ ì§œ ì°¾ê¸°
        date_span = soup.find('span', class_='date')
        date_text = date_span.get_text(strip=True) if date_span else ""

        return {
            'title': title_text,
            'content': content_text,
            'date': date_text,
            'url': url
        }

    except Exception as e:
        print(f"âš ï¸  í¬ë¡¤ë§ ì˜¤ë¥˜ ({url}): {e}")
        return None

def create_pdf(post_data, output_dir):
    """ê¸€ì„ PDFë¡œ ì €ì¥"""
    if not post_data:
        return False

    try:
        pdf = DhammaPDF()
        pdf.add_page()

        if pdf.font_available:
            pdf.set_font('NanumGothic', '', 16)

        # ì œëª©
        pdf.multi_cell(0, 10, post_data['title'])
        pdf.ln(5)

        # ë‚ ì§œ
        if post_data['date']:
            if pdf.font_available:
                pdf.set_font('NanumGothic', '', 10)
            pdf.cell(0, 10, post_data['date'], 0, 1)
            pdf.ln(5)

        # ë‚´ìš©
        if pdf.font_available:
            pdf.set_font('NanumGothic', '', 12)

        # ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì—¬ëŸ¬ ì¤„ë¡œ ë‚˜ëˆ„ê¸°
        for paragraph in post_data['content'].split('\n'):
            if paragraph.strip():
                pdf.multi_cell(0, 8, paragraph)
                pdf.ln(2)

        # íŒŒì¼ëª… ìƒì„± (íŠ¹ìˆ˜ë¬¸ì ì œê±°)
        safe_title = re.sub(r'[^\w\s-]', '', post_data['title'])[:50]
        filename = f"{safe_title}.pdf"
        filepath = os.path.join(output_dir, filename)

        pdf.output(filepath)
        return True

    except Exception as e:
        print(f"âš ï¸  PDF ìƒì„± ì˜¤ë¥˜: {e}")
        return False

def main():
    base_url = "http://www.dhamma.kr/wp/"
    output_dir = "/Users/jinseulpark/Desktop/github/jsks_app/scraper/pdfs"

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(output_dir, exist_ok=True)

    print("ğŸš€ Dhamma.kr í¬ë¡¤ë§ ì‹œì‘\n")

    # 1. ëª¨ë“  ê¸€ ë§í¬ ìˆ˜ì§‘
    post_links = get_all_post_links(base_url)

    if not post_links:
        print("âŒ ê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 2. ê° ê¸€ í¬ë¡¤ë§ ë° PDF ìƒì„±
    print(f"\nğŸ“„ PDF ìƒì„± ì‹œì‘...\n")

    success_count = 0
    for i, link in enumerate(post_links, 1):
        print(f"[{i}/{len(post_links)}] {link}")

        post_data = scrape_post_content(link)
        if post_data and create_pdf(post_data, output_dir):
            success_count += 1
            print(f"   âœ… PDF ì €ì¥ ì™„ë£Œ: {post_data['title'][:30]}...")
        else:
            print(f"   âŒ ì‹¤íŒ¨")

        # ì„œë²„ ë¶€í•˜ ë°©ì§€
        import time
        time.sleep(1)

    print(f"\nâœ¨ ì™„ë£Œ! {success_count}/{len(post_links)}ê°œì˜ PDF ìƒì„±ë¨")
    print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {output_dir}")

if __name__ == "__main__":
    main()

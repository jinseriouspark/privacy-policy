#!/usr/bin/env python3
"""
Dhamma.kr í…ìŠ¤íŠ¸ ì „ìš© í¬ë¡¤ëŸ¬ - ëª¨ë“  ê¸€ì„ TXTë¡œ ë¹ ë¥´ê²Œ ì €ì¥
"""

import requests
from bs4 import BeautifulSoup
import urllib3
import os
import re
import time

# SSL ê²½ê³  ë¬´ì‹œ
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

def get_all_post_links(base_url, max_pages=3368):
    """ëª¨ë“  ê¸€ì˜ ë§í¬ ìˆ˜ì§‘"""
    print("ğŸ“¡ ê¸€ ëª©ë¡ ìˆ˜ì§‘ ì¤‘...")

    post_links = []
    page = 1

    while page <= max_pages:
        url = f"{base_url}?paged={page}" if page > 1 else base_url
        print(f"   í˜ì´ì§€ {page}/{max_pages} í™•ì¸ ì¤‘...", end='\r')

        try:
            response = requests.get(url, verify=False, timeout=10)
            if response.status_code != 200:
                print(f"\n   âš ï¸  í˜ì´ì§€ {page} ì ‘ê·¼ ì‹¤íŒ¨")
                break

            soup = BeautifulSoup(response.content, 'html.parser')
            posts = soup.find_all('div', class_='post')

            if not posts:
                print(f"\n   âš ï¸  í˜ì´ì§€ {page}ì—ì„œ ê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                break

            for post in posts:
                title_link = post.find('a', class_='title')
                if title_link and title_link.get('href'):
                    href = title_link['href']
                    if href not in post_links:
                        post_links.append(href)

            page += 1
            time.sleep(0.3)  # PDFë³´ë‹¤ ë¹ ë¥´ê²Œ

        except Exception as e:
            print(f"\n   âš ï¸  í˜ì´ì§€ {page} ì˜¤ë¥˜: {e}")
            page += 1
            continue

    print(f"\nâœ… ì´ {len(post_links)}ê°œì˜ ê¸€ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
    return post_links

def scrape_post_content(url):
    """ê°œë³„ ê¸€ ë‚´ìš© í¬ë¡¤ë§"""
    try:
        response = requests.get(url, verify=False, timeout=10)
        soup = BeautifulSoup(response.content, 'html.parser')

        # ì œëª©
        title = soup.find('h2')
        title_text = title.get_text(strip=True) if title else "ì œëª© ì—†ìŒ"

        # ë‚´ìš©
        post_div = soup.find('div', class_='post')
        if post_div:
            paragraphs = post_div.find_all('p')
            content_paragraphs = paragraphs[1:-1] if len(paragraphs) > 2 else paragraphs
            content_text = '\n\n'.join([p.get_text(strip=True) for p in content_paragraphs if p.get_text(strip=True)])
        else:
            content_text = ""

        # ë‚ ì§œ
        date_span = soup.find('span', class_='date')
        date_text = date_span.get_text(strip=True) if date_span else ""

        return {
            'title': title_text,
            'content': content_text,
            'date': date_text,
            'url': url
        }

    except Exception as e:
        return None

def save_as_text(post_data, output_dir):
    """í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥"""
    if not post_data:
        return False

    try:
        # íŒŒì¼ëª… ìƒì„±
        safe_title = re.sub(r'[^\w\s-]', '', post_data['title'])[:50]
        filename = f"{safe_title}.txt"
        filepath = os.path.join(output_dir, filename)

        # í…ìŠ¤íŠ¸ íŒŒì¼ ì‘ì„±
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(f"ì œëª©: {post_data['title']}\n")
            f.write(f"ë‚ ì§œ: {post_data['date']}\n")
            f.write(f"URL: {post_data['url']}\n")
            f.write("\n" + "="*80 + "\n\n")
            f.write(post_data['content'])

        return True

    except Exception as e:
        return False

def main():
    base_url = "http://www.dhamma.kr/wp/"
    output_dir = "/Users/jinseulpark/Desktop/github/jsks_app/scraper/texts"

    os.makedirs(output_dir, exist_ok=True)

    print("ğŸš€ Dhamma.kr í…ìŠ¤íŠ¸ ì „ìš© í¬ë¡¤ë§ ì‹œì‘\n")
    print("ğŸ“Œ 3,368 í˜ì´ì§€ ì „ì²´ í¬ë¡¤ë§ (ì˜ˆìƒ ì‹œê°„: 1-2ì‹œê°„)\n")

    # 1. ëª¨ë“  ê¸€ ë§í¬ ìˆ˜ì§‘
    post_links = get_all_post_links(base_url)

    if not post_links:
        print("âŒ ê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 2. ê° ê¸€ í¬ë¡¤ë§ ë° TXT ìƒì„±
    print(f"\nğŸ“„ í…ìŠ¤íŠ¸ íŒŒì¼ ìƒì„± ì‹œì‘...\n")

    success_count = 0
    fail_count = 0

    for i, link in enumerate(post_links, 1):
        post_data = scrape_post_content(link)
        if post_data and save_as_text(post_data, output_dir):
            success_count += 1
            print(f"[{i}/{len(post_links)}] âœ… {post_data['title'][:40]}...", end='\r')
        else:
            fail_count += 1

        # ë¹ ë¥¸ í¬ë¡¤ë§
        time.sleep(0.5)

        # 100ê°œë§ˆë‹¤ ì§„í–‰ ìƒí™© ì¶œë ¥
        if i % 100 == 0:
            print(f"\nğŸ“Š ì§„í–‰ ìƒí™©: {success_count} ì„±ê³µ, {fail_count} ì‹¤íŒ¨")

    print(f"\n\nâœ¨ ì™„ë£Œ! {success_count}/{len(post_links)}ê°œì˜ í…ìŠ¤íŠ¸ íŒŒì¼ ìƒì„±ë¨")
    print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {output_dir}")

if __name__ == "__main__":
    main()
